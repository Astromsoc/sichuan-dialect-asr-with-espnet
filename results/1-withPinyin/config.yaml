config: conf/2_withPinyin.yaml
print_config: false
log_level: INFO
dry_run: false
iterator_type: sequence
output_dir: exp/asr_2_withPinyin_raw_sc_bpe350_sp
ngpu: 1
seed: 0
num_workers: 1
num_att_plot: 3
dist_backend: nccl
dist_init_method: env://
dist_world_size: null
dist_rank: null
local_rank: 0
dist_master_addr: null
dist_master_port: null
dist_launcher: null
multiprocessing_distributed: false
unused_parameters: false
sharded_ddp: false
cudnn_enabled: true
cudnn_benchmark: false
cudnn_deterministic: false
collect_stats: false
write_collected_feats: false
max_epoch: 100
patience: null
val_scheduler_criterion:
- valid
- loss
early_stopping_criterion:
- valid
- loss
- min
best_model_criterion:
-   - valid
    - acc
    - max
keep_nbest_models: 10
nbest_averaging_interval: 0
grad_clip: 3
grad_clip_type: 2.0
grad_noise: false
accum_grad: 1
no_forward_run: false
resume: true
train_dtype: float32
use_amp: true
log_interval: null
use_matplotlib: true
use_tensorboard: true
use_wandb: false
wandb_project: null
wandb_id: null
wandb_entity: null
wandb_name: null
wandb_model_log_interval: -1
detect_anomaly: false
pretrain_path: null
init_param: []
ignore_init_mismatch: false
freeze_param: []
num_iters_per_epoch: null
batch_size: 20
valid_batch_size: null
batch_bins: 10000000
valid_batch_bins: null
train_shape_file:
- exp/asr_stats_raw_sc_bpe350_sp/train/speech_shape
- exp/asr_stats_raw_sc_bpe350_sp/train/text_shape.bpe
valid_shape_file:
- exp/asr_stats_raw_sc_bpe350_sp/valid/speech_shape
- exp/asr_stats_raw_sc_bpe350_sp/valid/text_shape.bpe
batch_type: numel
valid_batch_type: null
fold_length:
- 80000
- 150
sort_in_batch: descending
sort_batch: descending
multiple_iterator: false
chunk_length: 500
chunk_shift_ratio: 0.5
num_cache_chunks: 1024
train_data_path_and_name_and_type:
-   - dump/raw/train_sc_sp/wav.scp
    - speech
    - sound
-   - dump/raw/train_sc_sp/text
    - text
    - text
valid_data_path_and_name_and_type:
-   - dump/raw/dev_sc/wav.scp
    - speech
    - sound
-   - dump/raw/dev_sc/text
    - text
    - text
allow_variable_data_keys: false
max_cache_size: 0.0
max_cache_fd: 32
valid_max_cache_size: null
optim: adam
optim_conf:
    lr: 4.0
scheduler: noamlr
scheduler_conf:
    model_size: 256
    warmup_steps: 25000
token_list:
- <blank>
- <unk>
- ▁me
- ▁de
- ▁shi
- ▁shen
- ▁you
- ▁yi
- ▁zen
- ▁ji
- ▁he
- ▁li
- ▁ma
- ▁zai
- ▁ge
- ▁zhi
- ▁wei
- ▁na
- ▁yang
- ▁ren
- ▁dian
- ▁zi
- ▁bu
- ▁hao
- ▁dao
- ▁ru
- ▁xi
- ▁jian
- ▁ke
- ▁da
- ▁wo
- ▁qi
- ▁zhong
- ▁shang
- ▁yu
- ▁chu
- ▁wang
- ▁wen
- ▁qu
- ▁wu
- ▁yao
- ▁ban
- ▁neng
- ▁a
- ▁nei
- ▁ying
- ▁duo
- ▁qing
- ▁xue
- ▁hui
- ▁xiang
- ▁jing
- ▁sheng
- ▁xian
- ▁yong
- ▁xie
- ▁jie
- ▁zuo
- ▁shu
- ▁er
- ▁guo
- ▁xing
- ▁jia
- ▁fa
- ▁xia
- ▁di
- ▁le
- ▁jiao
- ▁zui
- ▁shao
- ▁shou
- ▁gong
- ▁jin
- ▁hua
- ▁xiao
- ▁tian
- ▁dong
- ▁yuan
- ▁fu
- ▁hai
- ▁si
- ▁fang
- ▁zhu
- ▁bi
- ▁guan
- i
- ▁tong
- ▁bei
- ▁hou
- ▁mai
- ▁mei
- ▁ye
- ▁cheng
- ▁nan
- ▁xin
- ▁du
- ▁yan
- ▁ming
- ▁qian
- ▁chang
- ng
- ▁yin
- ▁gai
- ▁ju
- ou
- ▁cai
- ▁shui
- ▁nv
- ▁fei
- ▁jiu
- ▁ti
- ▁gao
- ▁bao
- ▁lian
- ▁nao
- ▁zheng
- ▁mian
- ▁huo
- ▁ba
- ▁su
- ▁dai
- ▁zhao
- ▁zhe
- ▁bai
- ▁gu
- ▁fen
- ▁zhan
- ▁ya
- ▁kan
- ▁zhang
- ▁che
- o
- ▁xu
- ▁ni
- ▁liao
- ▁zhen
- ▁wan
- g
- ▁tu
- ong
- ▁tai
- ▁pin
- ▁tou
- ▁bian
- ▁huan
- ▁yue
- ▁lu
- ▁hu
- ▁kai
- ▁zhuan
- ▁ai
- ▁liu
- ▁d
- ▁lai
- ▁cha
- ▁dui
- ▁zhou
- n
- ▁pian
- ▁an
- ▁ne
- ▁gan
- ▁shan
- ▁liang
- ▁qiu
- ▁nian
- ▁shuo
- ▁ka
- ▁sha
- ▁zhuang
- ▁dan
- ▁ben
- ▁kao
- ▁pi
- ▁mu
- ▁ci
- ang
- ▁biao
- ▁kuai
- ▁quan
- ▁jiang
- ▁she
- ▁mo
- ▁chuan
- ▁fan
- ▁ping
- ▁lei
- ▁bing
- ▁men
- ▁bie
- ▁c
- ▁ch
- ▁zu
- ▁han
- ▁hen
- ▁pai
- ▁lao
- e
- ▁chong
- ▁yun
- ▁cun
- ▁lv
- ▁ran
- ▁lan
- ▁luo
- ▁feng
- ▁mao
- ▁ting
- ▁se
- ▁ding
- ▁pan
- ▁suo
- ▁ta
- ▁san
- ▁tui
- ▁kong
- ▁ri
- ▁gou
- ▁zao
- ▁diao
- ▁ce
- ▁kou
- ▁wai
- ▁mi
- ▁bo
- ▁jue
- u
- ▁suan
- ▁gui
- ▁ku
- ▁la
- ▁ruan
- ▁ling
- ▁xiu
- ▁zong
- ▁man
- ▁xuan
- ▁hang
- ▁tiao
- ▁gua
- ▁chan
- ▁lun
- ▁l
- ▁hun
- ▁hong
- ▁te
- ▁min
- ▁rong
- ▁xun
- ▁huai
- ▁duan
- ao
- ▁gang
- ▁qiang
- ▁piao
- ▁meng
- ▁hei
- ▁p
- ▁tan
- ▁miao
- ▁pe
- ▁tang
- ▁dang
- ▁f
- ▁pei
- ▁r
- ▁lang
- ▁deng
- ▁kuan
- ▁zou
- ▁tie
- ▁huang
- ▁re
- ▁ze
- ▁lin
- ▁chuang
- ▁tao
- ▁s
- ▁pao
- ▁cao
- ▁cu
- ▁sui
- ▁nong
- ▁song
- an
- ▁jun
- ▁qin
- ▁tuan
- ▁po
- ▁xiong
- ▁can
- ▁chou
- ▁ha
- ▁za
- ▁sai
- ▁lie
- ▁juan
- ▁shua
- ▁kuang
- ▁e
- ▁ou
- ▁lou
- ▁cang
- ▁pang
- ▁chen
- ▁qiao
- ▁gen
- ▁teng
- ▁pen
- ▁wa
- ▁zhun
- ▁pa
- ▁die
- ▁bin
- ▁kun
- ▁kang
- ▁o
- ▁nai
- ▁ceng
- ▁leng
- ▁zang
- ▁shuai
- ▁fo
- ▁dun
- ▁zha
- ▁
- .
- v
- p
- k
- f
- t
- q
- r
- c
- b
- w
- l
- x
- j
- m
- d
- z
- y
- s
- h
- '-'
- /
- a
- <sos/eos>
init: null
input_size: null
ctc_conf:
    dropout_rate: 0.0
    ctc_type: builtin
    reduce: true
    ignore_nan_grad: true
joint_net_conf: null
use_preprocessor: true
token_type: bpe
bpemodel: data/sc_token_list/bpe_unigram350/bpe.model
non_linguistic_symbols: null
cleaner: null
g2p: null
speech_volume_normalize: null
rir_scp: null
rir_apply_prob: 1.0
noise_scp: null
noise_apply_prob: 1.0
noise_db_range: '13_15'
frontend: default
frontend_conf:
    n_fft: 512
    win_length: 400
    hop_length: 160
    fs: 16k
specaug: specaug
specaug_conf:
    apply_time_warp: true
    time_warp_window: 5
    time_warp_mode: bicubic
    apply_freq_mask: true
    freq_mask_width_range:
    - 0
    - 30
    num_freq_mask: 2
    apply_time_mask: true
    time_mask_width_range:
    - 0
    - 40
    num_time_mask: 2
normalize: global_mvn
normalize_conf:
    stats_file: exp/asr_stats_raw_sc_bpe350_sp/train/feats_stats.npz
model: espnet
model_conf:
    ctc_weight: 0.3
    lsm_weight: 0.1
    length_normalized_loss: false
preencoder: null
preencoder_conf: {}
encoder: conformer
encoder_conf:
    input_layer: conv2d
    num_blocks: 6
    linear_units: 1024
    dropout_rate: 0.3
    output_size: 256
    attention_heads: 4
    attention_dropout_rate: 0.0
    pos_enc_layer_type: rel_pos
    selfattention_layer_type: rel_selfattn
    activation_type: swish
    macaron_style: true
    use_cnn_module: true
    cnn_module_kernel: 15
postencoder: null
postencoder_conf: {}
decoder: transformer
decoder_conf:
    input_layer: embed
    num_blocks: 4
    linear_units: 1024
    dropout_rate: 0.3
required:
- output_dir
- token_list
version: 0.10.7a1
distributed: false
